{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "# import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "import wordninja\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "# TRY THIS OUT\n",
    "# https://github.com/nlptown/nlp-notebooks/blob/master/Simple%20Sentence%20Similarity.ipynb\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "import spacy_sentence_bert\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "# nlp_transform = spacy_sentence_bert.load_model('en_roberta_large_nli_stsb_mean_tokens')\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# links to check out\n",
    "# https://techblog.assignar.com/how-to-use-bert-sentence-embedding-for-clustering-text/\n",
    "# https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "# https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1\n",
    "# https://explosion.ai/blog/spacy-transformers\n",
    "# https://towardsdatascience.com/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"json_data.json\")\n",
    "\n",
    "allprompts = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain word bag\n",
    "\n",
    "prop_nouns = []\n",
    "start_words = []\n",
    "prop_nouns_spec = []\n",
    "\n",
    "prompt_corpus = []\n",
    "\n",
    "def get_bag(docs, out_list, word_list):\n",
    "    # out_list = []\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\") + [\"etc\", \"'s\", \"\"])\n",
    "\n",
    "    for include in [\"what\", \"where\", \"when\", \"why\", \"who\", \"how\"]:\n",
    "        stop_words.discard(include)\n",
    "        \n",
    "\n",
    "    for doc in docs:\n",
    "\n",
    "        if len(doc[1]) > 0 and len(doc[1][0]) > 0:\n",
    "            prompt_corpus.append(doc[1])\n",
    "        \n",
    "        # separated_words = word_tokenize(doc[1])\n",
    "        \n",
    "        tagged_list = nlp(doc[1])\n",
    "\n",
    "        prop_nouns_spec.append([(X.text, X.label_) for X in tagged_list.ents])\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_list = []\n",
    "\n",
    "        alpha_regex = re.compile('^[a-zA-Z]+$')\n",
    "\n",
    "        at_start = True\n",
    "\n",
    "        prev_prop = False\n",
    "\n",
    "        for wordtag in tagged_list:\n",
    "\n",
    "            word = str(wordtag)\n",
    "\n",
    "            if alpha_regex.match(word):\n",
    "                if at_start:\n",
    "                    start_words.append(word)\n",
    "                    at_start = False\n",
    "\n",
    "                # if len(word) > 1:\n",
    "                #     if not consecutiveUpper(word):\n",
    "                #         split_word = wordninja.split(word)\n",
    "\n",
    "                #         # print(split_word, word)\n",
    "\n",
    "                #         if len(split_word) > 1:\n",
    "                #             word = (split_word[0], wordtag.pos_)\n",
    "\n",
    "                #             for subword in split_word[1:]:\n",
    "                #                 tagged_list.append(subword)\n",
    "\n",
    "                if word not in stop_words:\n",
    "                    if len(word) > 1:\n",
    "                        pos = \"\"\n",
    "                        if wordtag.pos_ == \"PROPN\":\n",
    "                            if prev_prop:\n",
    "                                prop_nouns[-1] = prop_nouns[-1] + \" \" + word\n",
    "                            else:\n",
    "                                prop_nouns.append(word)\n",
    "                            prev_prop = True\n",
    "                        else:\n",
    "                            prev_prop= False\n",
    "                        \n",
    "                        if wordtag.pos_[-1] == \"J\":\n",
    "                            pos = \"a\"\n",
    "                        elif wordtag.pos_[-1] == \"N\":\n",
    "                            pos = \"n\"\n",
    "                        elif wordtag.pos_[0] == \"V\":\n",
    "                            pos = \"v\"\n",
    "                        elif wordtag.pos_[-1] == \"V\":\n",
    "                            pos = \"r\"\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                        lemmatized_list.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "                        # if lemmatizer.lemmatize(word, pos=pos) == \"u\":\n",
    "                        #     print(word, doc)\n",
    "            else:\n",
    "                if word in [\"?\", \".\", \"!\"]:\n",
    "                    at_start = True\n",
    "                continue\n",
    "\n",
    "            word_list += lemmatized_list\n",
    "            out_list.append(\" \".join(lemmatized_list))\n",
    "\n",
    "        \n",
    "    return out_list, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viche\\miniconda3\\envs\\scikit\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "numPrompts = []\n",
    "prompt_corpus_processed = []\n",
    "\n",
    "for college in allprompts:\n",
    "    numPrompts.append(len(college[1]))\n",
    "    \n",
    "    prompt_corpus_processed, word_list = get_bag(college[1], prompt_corpus_processed, word_list)\n",
    "    \n",
    "\n",
    "word_list = set(word_list)\n",
    "\n",
    "# for word in set(prop_nouns):\n",
    "#     if word.lower() in word_list:\n",
    "#         # print(word)\n",
    "#         word_list.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_vectors = []\n",
    "\n",
    "# i = 0\n",
    "\n",
    "# print(len(prompt_corpus))\n",
    "# print(prompt_corpus)\n",
    "\n",
    "for doc in nlp.pipe(prompt_corpus):\n",
    "    transformed_prompt = doc._.trf_data.tensors[-1]\n",
    "    if transformed_prompt.shape == (1,768):\n",
    "        prompt_vectors.append(transformed_prompt)\n",
    "\n",
    "prompt_matrix = np.stack(prompt_vectors, axis=1)\n",
    "\n",
    "\n",
    "# prompt_vectors[i]\n",
    "\n",
    "# print(allprompts[0][1])\n",
    "\n",
    "# for doc in nlp.pipe([\"some text\", \"some other text\"]):\n",
    "#     tokvecs = doc._.trf_data.tensors[-1]\n",
    "\n",
    "# tokvecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e054863a7f643d89d52cf81db31b10100e4d05b170cf47aaac02477aa3ba412"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('scikit': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
