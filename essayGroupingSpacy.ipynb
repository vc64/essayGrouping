{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# College Essay Prompt Clustering (with Spacy)\n",
    "\n",
    "The goal of this project is to group together similar supplemental essay prompts from colleges in the US. Ultimately, these groups should contain prompts similar enough to be responded to with very similar essays. \n",
    "\n",
    "This version of the project uses a Natural Language Processing(NLP) package called Spacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages and modules\n",
    "\n",
    "\n",
    "|Package name|Description|\n",
    "|------------|-----------|\n",
    "|numpy     |array-related actions, especially useful for 2D arrays       |\n",
    "|matplotlib|plotting/graphing       |\n",
    "|random    |set seed during KMeans (allows for reproducible results)  \n",
    "|sklearn   |data preprocessing and machine learning algorithms (aka scikit-learn)  \n",
    "|json      |raw prompt storage and retrieval  \n",
    "|spacy     |NLP tools, like transformers (sentence --> vector)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import json\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in prompts and storing them\n",
    "\n",
    "**allprompts**: A list of lists that stores all the prompts. In the overall list, each element is a list that represents a school. In each school's list, there is a unique number that corresponds to them and a list of tuples with each prompt and its title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"json_data.json\")\n",
    "\n",
    "allprompts = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "**numPrompts**: number of prompts from each school  \n",
    "**prompt_corpus**: prompts (sentences), no longer grouped by school  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numPrompts = []\n",
    "prompt_corpus = []\n",
    "\n",
    "for college in allprompts:\n",
    "    numPrompts.append(len(college[1]))\n",
    "\n",
    "    for prompt in college[1]:\n",
    "        if len(prompt[1]) > 0 and len(prompt[1][0]) > 0:\n",
    "            prompt_corpus.append(prompt[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Prompts into Vectors\n",
    "\n",
    "Spacy takes the `prompt_corpus` and performs an entire NLP process on them. nlp.pipe() returns an object with all the \"docs\" (prompts) processed. However, I just need the vector forms of the prompts, which can be accessed through `doc._trf_data.tensors[-1]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed vectors extracted from nlp.pipe()\n",
    "prompt_vectors = []\n",
    "\n",
    "# the original index of the prompt in the prompt_corpus\n",
    "prompt_indices = []\n",
    "i = 0\n",
    "\n",
    "for doc in nlp.pipe(prompt_corpus):\n",
    "    transformed_prompt = doc._.trf_data.tensors[-1]\n",
    "    \n",
    "    # shape is checked since some prompts are too long and are then transformed into multiple vectors, which I have chosen to omit\n",
    "    if transformed_prompt.shape == (1,768):\n",
    "        prompt_vectors.append(transformed_prompt[0])\n",
    "        prompt_indices.append(i)\n",
    "    i += 1\n",
    "\n",
    "# numpy 2D array where each row is a prompt's corresponding vector form\n",
    "prompt_matrix = np.stack(prompt_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (number of prompts, number of features)\n",
    "print(prompt_matrix.shape)\n",
    "\n",
    "# adds a column of indices from prompt_indices to the front of the prompt_matrix\n",
    "prompt_matrix_indexed = np.hstack((np.array(prompt_indices)[:,None], prompt_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra column now\n",
    "prompt_matrix_indexed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert from the index in the prompt_corpus to the list indices in allprompts\n",
    "# allows for conversion from vector back to original prompt only using index from first column\n",
    "\n",
    "def indToDict(index):\n",
    "    numPrompts = []\n",
    "    for prompts in allprompts:\n",
    "        numPrompts.append(len(prompts[1]))\n",
    "    \n",
    "    total = 0\n",
    "    for x in range(len(numPrompts)):\n",
    "        curr = numPrompts[x]\n",
    "\n",
    "        if (total + curr) > index:\n",
    "            ind = index - total\n",
    "            return (x, int(ind))\n",
    "        \n",
    "        total += curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indToDict(943)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 4 components\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "pca_data = pca.fit_transform(prompt_matrix_indexed[:,1:])\n",
    "\n",
    "# again, adds index column \n",
    "pca_data = np.hstack((np.arange(pca_data.shape[0])[:, None], pca_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_comps = pca_data.shape[1]-1\n",
    "\n",
    "fig, axs = plt.subplots(num_comps, num_comps)\n",
    "\n",
    "fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "\n",
    "fig.set_dpi(100)\n",
    "\n",
    "for i in range(1, num_comps+1):\n",
    "    for j in range(1, num_comps+1):\n",
    "        axs[i-1][j-1].scatter(pca_data[:,i]+1, pca_data[:,j]+1)\n",
    "        if i == 1:\n",
    "            axs[i-1][j-1].set_title(\"PC \" + str(j))\n",
    "        if j == 1:\n",
    "            axs[i-1][j-1].set_ylabel(\"PC \" + str(i), fontsize = \"large\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "\n",
    "pca_data = pca.fit_transform(prompt_matrix_indexed[:,1:])\n",
    "\n",
    "pca_data = np.hstack((np.arange(pca_data.shape[0])[:, None], pca_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "\n",
    "cumulative_var = [sum(pca.explained_variance_ratio_[0:x+1]) for x in range(len(pca.explained_variance_ratio_))]\n",
    "\n",
    "i=0\n",
    "for x in cumulative_var:\n",
    "    if x > 0.85:\n",
    "        print(i)\n",
    "        break\n",
    "    i+=1\n",
    "        \n",
    "\n",
    "plt.plot(PC_values, cumulative_var, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means for any dim, points is list of vectors in n-dim\n",
    "\n",
    "def select_init(points, k, labels):\n",
    "    random.seed = (333)\n",
    "\n",
    "    dist_weights = []\n",
    "    centroids = []\n",
    "\n",
    "    centroids.append(np.asarray(random.choice(points)))\n",
    "\n",
    "    while len(centroids) < k:\n",
    "        dist_weights = find_distances(points, centroids, labels)\n",
    "        centroids.append(np.asarray(random.choices(points, weights=dist_weights)[0]))\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def find_distances(points, centroids, labels = 0):\n",
    "    clusters = [[] for x in range(len(centroids))]\n",
    "    closest_dists = []\n",
    "\n",
    "    for curr_point in points:\n",
    "\n",
    "        # should be large enough given that words will rarely \n",
    "        #   occur more than even 20 times in one prompt\n",
    "        min_dist = 100000000\n",
    "        min_ind = len(centroids)\n",
    "        for j in range(len(centroids)):\n",
    "            curr_dist = sum((centroids[j][labels:] - curr_point[labels:]) ** 2) ** 0.5\n",
    "            \n",
    "            if curr_dist < min_dist:\n",
    "                min_dist = curr_dist\n",
    "        \n",
    "        closest_dists.append(min_dist)\n",
    "        \n",
    "    return closest_dists\n",
    "\n",
    "\n",
    "def find_clusters(points, centroids, labels = 0):\n",
    "    clusters = [[] for x in range(len(centroids))]\n",
    "\n",
    "    for curr_point in points:\n",
    "\n",
    "        # should be large enough given that words will rarely \n",
    "        #   occur more than even 20 times in one prompt\n",
    "        min_dist = 100000000\n",
    "        min_ind = len(centroids)\n",
    "        for j in range(len(centroids)):\n",
    "            curr_dist = sum((centroids[j][labels:] - curr_point[labels:]) ** 2) ** 0.5\n",
    "            if curr_dist < min_dist:\n",
    "                min_dist = curr_dist\n",
    "                min_ind = j\n",
    "        \n",
    "        clusters[min_ind].append(curr_point)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def k_means(points, k, labels = 0):\n",
    "\n",
    "    print(labels)\n",
    "\n",
    "    centroids = select_init(points, k, labels)\n",
    "\n",
    "\n",
    "    for center_num in range(k):\n",
    "        centroids[center_num] = np.asarray(centroids[center_num])\n",
    "    \n",
    "    equal_means = 0\n",
    "    clusters = []\n",
    "\n",
    "    while equal_means < k:\n",
    "        clusters = find_clusters(points, centroids, labels)\n",
    "        \n",
    "        for i in range(k):\n",
    "            mean = sum(np.asarray(clusters[i])) / len(clusters[i])\n",
    "\n",
    "            dim = labels\n",
    "            curr_equal_means = True\n",
    "            while (dim < len(points[0])) and curr_equal_means:\n",
    "                if (mean[dim] != centroids[i][dim]):\n",
    "                    centroids[i] = mean\n",
    "                    curr_equal_means = False\n",
    "                dim += 1\n",
    "            \n",
    "            if dim == len(points[0]):\n",
    "                equal_means += 1\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_points = [list(pca_data[i,0:3]) for i in range(len(pca_data[:,1]))]\n",
    "\n",
    "num_clusters = 30\n",
    "\n",
    "pca_clusters = k_means(pca_points, num_clusters, labels = 1)\n",
    "\n",
    "print(len(pca_clusters[0]))\n",
    "\n",
    "pca_clusters_graph = []\n",
    "\n",
    "for c in range(len(pca_clusters)):\n",
    "    for x in pca_clusters[c]:\n",
    "        pca_clusters_graph.append(x + [c])\n",
    "\n",
    "pca_clusters_graph = np.asarray(pca_clusters_graph)\n",
    "\n",
    "plt.scatter(pca_clusters_graph[:,1], pca_clusters_graph[:,2], c = pca_clusters_graph[:,3])\n",
    "plt.xlabel(\"PC 1\") \n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.title(\"PC 1 vs PC 2 with 2D KMeans clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "print(len(pca_clusters[n]))\n",
    "for point in pca_clusters[n]:\n",
    "    r, c = indToDict(point[0])\n",
    "\n",
    "    print(allprompts[int(r)][1][int(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn KMeans function\n",
    "elbow = []\n",
    "\n",
    "for i in range(1, 50):\n",
    "    kmeans = KMeans(n_clusters = i, init = \"k-means++\", random_state = 333)\n",
    "    kmeans.fit(prompt_matrix_indexed[:,1:])\n",
    "    elbow.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(elbow)\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"\")\n",
    "plt.title(\"KMeans Elbow Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 50, init = \"k-means++\", random_state = 333)\n",
    "\n",
    "kmeans.fit(prompt_matrix_indexed[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_.shape\n",
    "\n",
    "plt.scatter(pca_data[:,1], pca_data[:,2], c = kmeans.labels_)\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.title(\"PC 1 vs PC 2 plot with multidimensional KMeans clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "\n",
    "count = 0\n",
    "for p in range(len(kmeans.labels_)):\n",
    "    if int(kmeans.labels_[p]) == n:\n",
    "        r, c = indToDict(p)\n",
    "\n",
    "        print(allprompts[int(r)][1][int(c)])\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clusters = k_means(prompt_matrix_indexed, 50, labels = 1)\n",
    "\n",
    "for x in all_clusters:\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "for x in all_clusters[n:n+1]:\n",
    "    print(len(x))\n",
    "\n",
    "    for i in range(0, len(x)):\n",
    "        r, c = indToDict(x[i][0])\n",
    "\n",
    "        print(allprompts[int(r)][1][int(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "cluster_obj = hdbscan.HDBSCAN(min_cluster_size=5, min_samples = 1)\n",
    "\n",
    "# cluster_obj = hdbscan.HDBSCAN()\n",
    "\n",
    "cluster_obj.fit(prompt_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum([x for x in cluster_obj.labels_ if x == -1]))\n",
    "print(max(cluster_obj.labels_[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(333)\n",
    "\n",
    "projection = sklearn.manifold.TSNE().fit_transform(prompt_matrix)\n",
    "plt.scatter(*projection.T, c=cluster_obj.labels_)\n",
    "\n",
    "plt.xlabel(\"tSNE Dim 1\")\n",
    "plt.ylabel(\"tSNE Dim 2\")\n",
    "plt.title(\"tSNE plot colored by HDBSCAN clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_comps = 6\n",
    "\n",
    "fig, axs = plt.subplots(num_comps-1, num_comps-1)\n",
    "\n",
    "fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "\n",
    "fig.set_dpi(100)\n",
    "\n",
    "for i in range(1, num_comps):\n",
    "    for j in range(1, num_comps):\n",
    "        axs[i-1][j-1].scatter(pca_data[:,i]+1, pca_data[:,j]+1, c = [\"red\" if x < 0 else \"blue\" for x in cluster_obj.labels_], s=[1 if x < 0 else 1 for x in cluster_obj.labels_])\n",
    "        if i == 1:\n",
    "            axs[i-1][j-1].set_title(\"PC \" + str(j))\n",
    "        if j == 1:\n",
    "            axs[i-1][j-1].set_ylabel(\"PC \" + str(i), fontsize = \"large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e054863a7f643d89d52cf81db31b10100e4d05b170cf47aaac02477aa3ba412"
  },
  "kernelspec": {
   "display_name": "Python [conda env:scikit] *",
   "language": "python",
   "name": "conda-env-scikit-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
